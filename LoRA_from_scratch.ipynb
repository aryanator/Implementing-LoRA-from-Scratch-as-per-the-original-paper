{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets"
      ],
      "metadata": {
        "id": "UPcuWacg_Tvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCJW3AH6-gWl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel\n",
        "\n",
        "# -----------------------------\n",
        "# LoRA Module (based on Hu et al. 2021)\n",
        "# -----------------------------\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, r, alpha, layer):\n",
        "        \"\"\"\n",
        "        Implements LoRA injection: W + ΔW, where ΔW = BA, rank r\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_layer = layer                     # Original frozen weight (e.g., query or value)\n",
        "        self.scale = alpha / r                      # LoRA scaling factor (from paper)\n",
        "\n",
        "        self.n = layer.in_features\n",
        "        self.m = layer.out_features\n",
        "\n",
        "        # Low-rank trainable matrices\n",
        "        self.B = nn.Parameter(torch.zeros(self.n, r))    # B ∈ R^{in_features × r}\n",
        "        self.A = nn.Parameter(torch.rand(r, self.m))     # A ∈ R^{r × out_features}\n",
        "\n",
        "        self.merged = False\n",
        "\n",
        "        # Dropout on the delta path (optional; improves generalization)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Freeze the base layer's weights\n",
        "        for param in self.base_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        base = self.base_layer(x)                         # frozen forward\n",
        "        delta = F.linear(x, torch.matmul(self.B, self.A)) # LoRA delta\n",
        "        delta = self.dropout(delta)                       # optional dropout\n",
        "        return base + self.scale * delta                  # Inject LoRA\n",
        "\n",
        "    def merge(self):\n",
        "        \"\"\"\n",
        "        Merges LoRA weights into base weights (for inference).\n",
        "        Only needed if you want to export or remove dependency on LoRA modules.\n",
        "        \"\"\"\n",
        "        if not self.merged:\n",
        "            merged = torch.matmul(self.B, self.A).T       # Match shape [out_features, in_features]\n",
        "            self.base_layer.weight.data += self.scale * merged\n",
        "            self.merged = True\n",
        "\n",
        "# -----------------------------\n",
        "# LoRA Applied to BERT (query/value)\n",
        "# -----------------------------\n",
        "class loraBERT(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        \"\"\"\n",
        "        Wraps a BERT model and injects LoRA adapters into attention (query and value).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Inject LoRA into each transformer layer (Q and V only, per original paper)\n",
        "        for layer in self.model.encoder.layer:\n",
        "            q = layer.attention.self.query\n",
        "            v = layer.attention.self.value\n",
        "            layer.attention.self.query = LoRA(r=4, alpha=10, layer=q)\n",
        "            layer.attention.self.value = LoRA(r=4, alpha=10, layer=v)\n",
        "\n",
        "        # Freeze all model parameters by default\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze LoRA weights only\n",
        "        for module in self.model.modules():\n",
        "            if isinstance(module, LoRA):\n",
        "                for param in module.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        # Task-specific classifier head\n",
        "        self.classifier = nn.Linear(self.model.config.hidden_size, num_classes)\n",
        "\n",
        "    def count_params(self):\n",
        "        \"\"\"\n",
        "        Returns number of trainable LoRA params vs total model size.\n",
        "        \"\"\"\n",
        "        lora = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        total = sum(p.numel() for p in self.model.parameters())\n",
        "        return {'lora_params': lora, 'total_params': total}\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = output.last_hidden_state[:, 0, :]  # CLS token\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "# -----------------------------\n",
        "# LoRA-augmented MLP (example outside transformers)\n",
        "# -----------------------------\n",
        "class loraMLP(nn.Module):\n",
        "    def __init__(self, n, m, h):\n",
        "        \"\"\"\n",
        "        LoRA applied to MLP layers.\n",
        "        Args:\n",
        "          m: input_dim\n",
        "          h: hidden_dim\n",
        "          n: output_dim\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        fc1 = nn.Linear(m, h)\n",
        "        fc2 = nn.Linear(h, n)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.new_fc1 = LoRA(4, 10, fc1)\n",
        "        self.new_fc2 = LoRA(4, 10, fc2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = self.relu(self.new_fc1(x))\n",
        "        return self.new_fc2(h1)\n",
        "\n",
        "# -----------------------------\n",
        "# Instantiation Example\n",
        "# -----------------------------\n",
        "model_name = 'bert-base-uncased'\n",
        "lora_bert = loraBERT(model_name, num_classes=2)\n",
        "\n",
        "lora_mlp = loraMLP(n=10, m=2, h=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup ===\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Load tokenizer and dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataset = load_dataset('glue', 'sst2')\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch['sentence'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Apply tokenizer to dataset\n",
        "encoded = dataset.map(tokenize_fn, batched=True)\n",
        "encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(encoded['train'], batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(encoded['validation'], batch_size=32)\n",
        "\n",
        "# Instantiate LoRA-BERT model\n",
        "model = loraBERT('bert-base-uncased', num_classes=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer (only trainable LoRA + classifier weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# === Validation ===\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Validation Accuracy: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "id": "hqXR2QzC_7Ss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}